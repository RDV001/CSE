{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZE8H-gouBZT"
   },
   "source": [
    "# Wikispeedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nx_parallel as nxp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as cm\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "from joblib import parallel_config, Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable networkx's config for nx-parallel and set global configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.config.backends.parallel.active = True\n",
    "\n",
    "nxp_config = nx.config.backends.parallel\n",
    "nxp_config.n_jobs = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMJIO5HUuBKe"
   },
   "source": [
    "Read graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2086,
     "status": "ok",
     "timestamp": 1732382700947,
     "user": {
      "displayName": "Riccardo De Vidi",
      "userId": "17191632774646059225"
     },
     "user_tz": -60
    },
    "id": "QD_yVi1vYLrl",
    "outputId": "d6fbfb39-8c2a-4ba1-bb35-70759157b927",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = pd.read_csv('wikispeedia_paths-and-graph/links.tsv', delimiter='\\t', names=['linkSource', 'linkTarget'], header=None, comment=\"#\")\n",
    "articles = pd.read_csv('wikispeedia_paths-and-graph/articles.tsv', delimiter='\\t', names=['article'], header=None, comment=\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished = pd.read_csv('wikispeedia_paths-and-graph/paths_finished.tsv', delimiter='\\t', usecols=[3], names=['path'], header=None, comment=\"#\")\n",
    "paths_finished['target'] = paths_finished['path'].apply(lambda x: x.split(';')[-1])\n",
    "paths_finished['path'] = paths_finished['path'].apply(lambda x: x.split(';'))\n",
    "\n",
    "paths_unfinished = pd.read_csv('wikispeedia_paths-and-graph/paths_unfinished.tsv', delimiter='\\t', usecols=[3, 4], names=['path', 'target'], header=None, comment=\"#\")\n",
    "paths_unfinished['path'] = paths_unfinished['path'].apply(lambda x: x.split(';'))\n",
    "\n",
    "paths = pd.concat([paths_finished, paths_unfinished], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter game data to discard '<'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paths = [] \n",
    "    \n",
    "for p in paths['path']:\n",
    "    indices  = [index for (index, item) in enumerate(p) if item == '<']\n",
    "    filter_indices = []; \n",
    "    for index in indices: filter_indices.extend([index - 1, index])\n",
    "    filtered_paths.append([item for index, item in enumerate(p) if index not in filter_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7naPe1MauYjH"
   },
   "source": [
    "Generate the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 91549,
     "status": "ok",
     "timestamp": 1732383083115,
     "user": {
      "displayName": "Riccardo De Vidi",
      "userId": "17191632774646059225"
     },
     "user_tz": -60
    },
    "id": "UL0fkbDoZFB1",
    "outputId": "480469ba-f893-4c1a-8373-524dbef86e55"
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(links, source='linkSource', target='linkTarget', create_using=nx.DiGraph)\n",
    "disconnected_nodes = set(articles['article'].tolist()) - set(G.nodes)\n",
    "G.add_nodes_from(disconnected_nodes)\n",
    "\n",
    "H = nxp.ParallelGraph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central article usage by players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py_j2bTbLkbL"
   },
   "source": [
    "Compute the shortest-path betweenness centrality for nodes (not approximated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_bc = nxp.betweenness_centrality(H);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article usage by players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_bc = sorted(nodes_bc.items(), key=lambda item: item[1], reverse=True)\n",
    "percentage_bc = []\n",
    "percentage_l = []\n",
    "for i in range(1, 100):\n",
    "    percentage_bc.append(sorted_bc[i][1])\n",
    "    percentage_l.append(sorted_bc[i][0])\n",
    "\n",
    "percentage_finished = []\n",
    "for i in range(1, 100):\n",
    "    count = 0\n",
    "    for path in paths_finished['path']:\n",
    "        if sorted_bc[i][0] in path:\n",
    "            count += 1\n",
    "    percentage_finished.append(count/len(paths_finished['path']))\n",
    "print()\n",
    "\n",
    "percentage_unfinished = []\n",
    "for i in range(1, 100):\n",
    "    count = 0\n",
    "    for path in paths_unfinished['path'][1:-1]:\n",
    "        if sorted_bc[i][0] in path:\n",
    "            count += 1\n",
    "    percentage_unfinished.append(count/len(paths_unfinished['path']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 17\n",
    "MEDIUM_SIZE = 17\n",
    "BIGGER_SIZE = 17\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "_ = plt.figure(figsize=(15, 5))\n",
    "_ = plt.hist(\n",
    "    [percentage_l, percentage_l, percentage_l],\n",
    "    weights=[percentage_bc, percentage_finished, percentage_unfinished],\n",
    "    align='left',\n",
    "    label=['Betweenness centralities', 'Finished paths values', 'Unfinished paths values'],\n",
    "    bins=100,\n",
    ")\n",
    "_ = plt.xticks([])\n",
    "_ = plt.legend()\n",
    "\n",
    "_ = plt.savefig('BetweennessHist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player paths analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_couples = []\n",
    "\n",
    "for index, row in paths.iterrows():\n",
    "    src = row['path'][0]\n",
    "    dst = row['target']\n",
    "    if(not [src, dst] in unique_couples):\n",
    "        unique_couples.append([src, dst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all shortest paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_paths_f(couple):\n",
    "    try:\n",
    "        shortest_paths_generator = nx.all_shortest_paths(G, couple[0], couple[1])\n",
    "        shortest_paths = list(shortest_paths_generator)\n",
    "        if not os.path.isdir('shortest_paths'):\n",
    "            os.makedirs('shortest_paths')\n",
    "        with open('shortest_paths/' + couple[0] + '#' + couple[1] + '.py', 'w') as out_file:\n",
    "            out_file.write(json.dumps(shortest_paths))\n",
    "    except Exception as e:\n",
    "        print(couple[0] + '#' + couple[1] + ' exception : ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to compute the shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(delayed(shortest_paths_f)(couple) for couple in unique_couples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compare_paths(player_path, optimal_paths, output_folder):\n",
    "   \n",
    "    best_beg_equal_nodes = 0\n",
    "    best_end_equal_nodes = 0\n",
    "    best_total_equal_nodes = 0\n",
    "        \n",
    "    for op in optimal_paths:\n",
    "\n",
    "        i = 0\n",
    "        while(i < len(op) and i < len(player_path) and player_path[i] == op[i]): i = i + 1\n",
    "        beg_equal_nodes = i\n",
    "            \n",
    "        i = 0\n",
    "        while(i > -len(op) and i > -len(player_path) and player_path[i - 1] == op[i - 1]): i = i - 1\n",
    "        end_equal_nodes = -i\n",
    "\n",
    "        total_equal_nodes = len(list(set(op).intersection(player_path)))\n",
    "\n",
    "        if beg_equal_nodes > best_beg_equal_nodes: best_beg_equal_nodes = beg_equal_nodes\n",
    "        if end_equal_nodes > best_end_equal_nodes: best_end_equal_nodes = end_equal_nodes\n",
    "        if total_equal_nodes > best_total_equal_nodes: best_total_equal_nodes = total_equal_nodes\n",
    "\n",
    "    try:\n",
    "        if not os.path.isdir(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        with open(output_folder + '/' + optimal_paths[0][0] + '#' + optimal_paths[0][-1] + '.txt', 'a') as out_file:\n",
    "            out_file.write(\n",
    "                str(len(optimal_paths[0])) + ' ' + \n",
    "                str(len(player_path)) + ' ' +\n",
    "                str(best_beg_equal_nodes) + ' ' + \n",
    "                str(best_end_equal_nodes) + ' ' + \n",
    "                str(best_total_equal_nodes) + ' ' +\n",
    "                str(int(player_path[-1] == optimal_paths[0][-1])) + '\\n'\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(optimal_paths[0][0] + '#' + optimal_paths[0][-1] + ' exception : ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in paths.iterrows():\n",
    "    optimal_paths = []\n",
    "    try:\n",
    "        with open('shortest_paths' + '/' + row['path'][0] + '#' + row['target'] + '.py') as in_file:\n",
    "            optimal_paths = json.load(in_file)\n",
    "        compare_paths(row['path'], optimal_paths, 'metrics')\n",
    "    except Exception as e:\n",
    "        print(row['path'][0] + '#' + row['target'] + ' exception : ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for couple in unique_couples:\n",
    "    try:\n",
    "        with open('metrics' + '/' + couple[0] + '#' + couple[1] + '.txt') as in_file:\n",
    "            file_lines = in_file.readlines()\n",
    "            for line in file_lines:\n",
    "                s = line.split() \n",
    "                v = int(s[0]) - 1  # len\n",
    "                w = int(s[1]) - 1  # player len\n",
    "                d = int(s[1]) - int(s[0]) # difference\n",
    "                x = max(int(s[2]) - 1, 0)  # beg equal edges \n",
    "                y = max(int(s[3]) - 1, 0)  # end equal edges\n",
    "                z = max(int(s[4]) - 1, 0)  # total equal edges\n",
    "                t = bool(int(s[5])) # finished?\n",
    "                data.append([v, w, d, x, y, z, t])\n",
    "    except Exception as e:\n",
    "        print(couple[0] + '#' + couple[1] + ' exception : ' + str(e))\n",
    "df = pd.DataFrame(data, columns = ['Optimal length', 'Player length', 'Difference', 'Beg equal edges', 'End equal edges', 'Tot equal edges', 'Finished']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 17\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "g = sns.PairGrid(df, hue='Finished', hue_order=[True, False], diag_sharey=False)\n",
    "g.map_diag(sns.histplot, binwidth=1, binrange=(0, 10), multiple='dodge', stat='density', common_norm=False, discrete=True)\n",
    "g.data = df[df['Finished'] == True]\n",
    "g.map_lower(sns.histplot, binwidth=1, binrange=(0, 10), discrete=True, stat='density')\n",
    "g.data = df[df['Finished'] == False]\n",
    "g.map_upper(sns.histplot, binwidth=1, binrange=(0, 10), discrete=True, stat='density')\n",
    "g.add_legend()\n",
    "\n",
    "_ = plt.savefig('ComparisonHue.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest paths lengths distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikispeedia lengths histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_dict_G = dict(nx.all_pairs_shortest_path_length(G))\n",
    "lengths_list_G = []\n",
    "for src in G.nodes:\n",
    "        for dst in G.nodes:\n",
    "            try:\n",
    "                lengths_list_G.append(lengths_dict_G[src][dst])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "max_len = 10\n",
    "hist_G = np.histogram(lengths_list_G, bins=range(max_len + 2), density=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random graphs histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_list = []\n",
    "mc_samples = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to generate the graphs and evaluate the distances between all the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mc_samples):\n",
    "    print(i)\n",
    "    GG = nx.directed_configuration_model([G.in_degree(node) for node in G], [G.out_degree(node) for node in G])\n",
    "    lengths_dict = dict(nx.all_pairs_shortest_path_length(GG))\n",
    "    sample_lengths = []\n",
    "    for src in GG.nodes:\n",
    "        for dst in GG.nodes:\n",
    "            try:\n",
    "                sample_lengths.append(lengths_dict[src][dst])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    lengths_list.append(sample_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_graphs_distances.py') as in_file:\n",
    "    lengths_list = json.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = []\n",
    "\n",
    "for l in lengths_list:\n",
    "    hists.append(np.histogram(l, bins=range(max_len + 2), density=True)[0])\n",
    "\n",
    "hists = np.array(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_mean = np.mean(hists, axis=0)\n",
    "hists_stds = np.std(hists, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = []\n",
    "for i in range(max_len + 1):\n",
    "    z_scores.append((hist_G[i] - hists_mean[i])/hists_stds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(lengths_list_G, bins=range(max_len + 1), density='True', align='left', alpha=0.5, label='Wikispeedia')\n",
    "_ = plt.hist(range(max_len + 1), weights=hists_mean, bins=range(max_len + 1), align='left', alpha=0.5, label='Random')\n",
    "_ = plt.xticks(range(max_len + 1))\n",
    "_ = plt.xlabel('Path length')\n",
    "_ = plt.ylabel('Density')\n",
    "_ = plt.legend()\n",
    "\n",
    "_ = plt.savefig('WikispeediaVSRandom.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis on the maximum, minimum, average and distribution of the users' paths length. This information will be used to understand how to create random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all path lengths\n",
    "p_fin_lengths = []\n",
    "sum_length = 0\n",
    "\n",
    "for f_path in paths_finished['path']:\n",
    "    l = len(f_path)\n",
    "    p_fin_lengths.append(l)\n",
    "    sum_length += l\n",
    "\n",
    "#calculate min, max, average lengths\n",
    "print(\"Min length:\", min(p_fin_lengths))\n",
    "print(\"Max length:\", max(p_fin_lengths))\n",
    "print(\"Average length:\", sum_length / len(p_fin_lengths))\n",
    "\n",
    "#plot the histogram of the path lengths frewuencies\n",
    "plt.hist(p_fin_lengths, bins = 80, range = (0, 30))\n",
    "plt.xlabel('Finished paths lengths')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Finished Paths Lengths Frequncies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Method to perform a random walk of at most n steps from a source and destrination node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_walk(g, source, destination, n):\n",
    "  crossed_nodes = []\n",
    "  current_node = source\n",
    "  crossed_nodes.append(current_node)\n",
    "  for i in range(n):\n",
    "    if current_node == destination:\n",
    "      break\n",
    "    list_neighbors = list(g.neighbors(current_node))\n",
    "    if len(list_neighbors)==0:\n",
    "      break\n",
    "    else:\n",
    "      current_node = random.choice(list_neighbors)\n",
    "      crossed_nodes.append(current_node)\n",
    "  return crossed_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the length of the users' paths and of the random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_user_random_paths_lengths(user_lengths, random_lengths, paths_count):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    offset = 0.15 \n",
    "\n",
    "    plt.bar(range(paths_count), user_lengths, width=0.1, color='blue', label=\"User Path\")\n",
    "\n",
    "    for i in range(paths_count):\n",
    "        for j, length in enumerate(random_lengths[i]):\n",
    "            if j > 1/offset - 1: \n",
    "                break\n",
    "            plt.bar(\n",
    "                i + (j + 1) * offset, \n",
    "                length,\n",
    "                width=0.1,\n",
    "                color='orange',\n",
    "                label=\"Random Path\" if i == 0 and j == 0 else \"\"\n",
    "            )\n",
    "\n",
    "    plt.title(\"User and Random paths lengths\", fontsize=16)\n",
    "    plt.xlabel(\"Attempts\", fontsize=14)\n",
    "    plt.ylabel(\"Path Length\", fontsize=14)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "    #plt.savefig('images/'+'plot'+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some random walks on a subset of source and destination nodes picked from the users's finished paths. Plot the results comparing the resulting path lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_finished_paths = 30\n",
    "number_of_random_attempts = 200\n",
    "max_steps = 15\n",
    "\n",
    "user_lengths = [] \n",
    "random_lengths = [[] for _ in range(number_of_finished_paths)]  \n",
    "\n",
    "for i in range(number_of_finished_paths):\n",
    "    random_i = random.choice(range(len(paths_finished)))\n",
    "    source, destination = paths_finished['path'][random_i][0], paths_finished['path'][random_i][-1]\n",
    "\n",
    "    #print(\"-\", source, \"-->\", destination)\n",
    "    \n",
    "    user_lengths.append(len(paths_finished['path'][random_i]))\n",
    "    \n",
    "    for j in range(number_of_random_attempts):\n",
    "        crossed_nodes = do_random_walk(G, source, destination, max_steps)\n",
    "        if crossed_nodes[-1] == destination:\n",
    "            random_lengths[i].append(len(crossed_nodes)) \n",
    "            #print(\"Number of steps:\", len(crossed_nodes), \" --> \", crossed_nodes)\n",
    "    #if len(random_lengths[i]) == 0:\n",
    "        #print(\"No path found\")\n",
    "\n",
    "plot_user_random_paths_lengths(user_lengths, random_lengths, number_of_finished_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random paths for the games in which users needed more than 50 steps. Plot the results comparing the resulting path lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of paths with length over 50\n",
    "count_over_50 = len([i for i in p_fin_lengths if i > 50])\n",
    "print(\"Total number of paths: \", len(p_fin_lengths))\n",
    "print(\"Number of paths with length over 50:\", count_over_50)\n",
    "\n",
    "number_of_random_attempts = 200\n",
    "max_steps = 50\n",
    "\n",
    "user_lengths = [] \n",
    "random_lengths = [[] for _ in range(count_over_50)]  \n",
    "\n",
    "i = 0\n",
    "for path in paths_finished['path']:\n",
    "    if(len(path) <= 50):\n",
    "        continue\n",
    "    \n",
    "    source, destination = path[0], path[-1]\n",
    "    \n",
    "    user_lengths.append(len(path))\n",
    "    \n",
    "    for j in range(number_of_random_attempts):\n",
    "        crossed_nodes = do_random_walk(G, source, destination, max_steps)\n",
    "        if crossed_nodes[-1] == destination:\n",
    "            random_lengths[i].append(len(crossed_nodes)) \n",
    "    i += 1\n",
    "\n",
    "plot_user_random_paths_lengths(user_lengths, random_lengths, count_over_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some random walks on a subset of source and destination nodes picked from the user's unfinished paths. Plot the results comparing the resulting path lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_finished_paths = 30\n",
    "number_of_random_attempts = 200\n",
    "max_steps = 15\n",
    "\n",
    "user_lengths = [] \n",
    "random_lengths = [[] for _ in range(number_of_finished_paths)]  \n",
    "\n",
    "for i in range(number_of_finished_paths):\n",
    "    random_i = random.choice(range(len(paths_unfinished)))\n",
    "    source, destination = paths_unfinished['path'][random_i][0], paths_unfinished['target'][random_i]\n",
    "\n",
    "    #print(\"-\", source, \"-->\", destination)\n",
    "    \n",
    "    user_lengths.append(len(paths_unfinished['path'][random_i]))\n",
    "    \n",
    "    for j in range(number_of_random_attempts):\n",
    "        crossed_nodes = do_random_walk(G, source, destination, max_steps)\n",
    "        if crossed_nodes[-1] == destination:\n",
    "            random_lengths[i].append(len(crossed_nodes)) \n",
    "            #print(\"Number of steps:\", len(crossed_nodes), \" --> \", crossed_nodes)\n",
    "    #if len(random_lengths[i]) == 0:\n",
    "        #print(\"No path found\")\n",
    "\n",
    "plot_user_random_paths_lengths(user_lengths, random_lengths, number_of_finished_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the number of paths in which the random walks do not find a connection between source and destination in 200 attempts with at most 20 steps using the source and destination nodes of the finished paths. (about 9 minutes to compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_random_attempts = 200\n",
    "max_steps = 20\n",
    "\n",
    "user_lengths = [] \n",
    "random_lengths = [[] for _ in range(len(paths_finished))]  \n",
    "\n",
    "for i in range(len(paths_finished)):\n",
    "    source, destination = paths_finished['path'][i][0], paths_finished['path'][i][-1]\n",
    "\n",
    "    user_lengths.append(len(paths_finished['path'][i]))\n",
    "    \n",
    "    for j in range(number_of_random_attempts):\n",
    "        crossed_nodes = do_random_walk(G, source, destination, max_steps)\n",
    "        if crossed_nodes[-1] == destination:\n",
    "            random_lengths[i].append(len(crossed_nodes)) \n",
    "\n",
    "num_unfinished_random_paths = len([i for i in random_lengths if len(i) == 0])\n",
    "print(\"Number of games without a finished random path: \", num_unfinished_random_paths)\n",
    "print(\"Total number of paths: \", len(paths_finished))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the number of paths in which the random walks do not find a connection between source and destination in 200 attempts with at most 20 steps using the source and destination nodes of the unfinished paths. (about 5 minutes to compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_random_attempts = 200\n",
    "max_steps = 20\n",
    "\n",
    "user_lengths = [] \n",
    "random_lengths = [[] for _ in range(len(paths_unfinished))]  \n",
    "\n",
    "for i in range(len(paths_unfinished)):\n",
    "    source, destination = paths_unfinished['path'][i][0], paths_unfinished['target'][i]\n",
    "    \n",
    "    user_lengths.append(len(paths_unfinished['path'][i]))\n",
    "    \n",
    "    for j in range(number_of_random_attempts):\n",
    "        crossed_nodes = do_random_walk(G, source, destination, max_steps)\n",
    "        if crossed_nodes[-1] == destination:\n",
    "            random_lengths[i].append(len(crossed_nodes)) \n",
    "\n",
    "num_unfinished_random_paths = len([i for i in random_lengths if len(i) == 0])\n",
    "print(\"Number of games without a finished random path: \", num_unfinished_random_paths)\n",
    "print(\"Total number of paths: \", len(paths_unfinished))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_clustering = nx.algorithms.community.greedy_modularity_communities(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clust in enumerate(greedy_clustering):\n",
    "    print(f\"Cluster {i+1}: {sorted(clust)} \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Louvian Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louv_clustering = nx.algorithms.community.louvain_communities(G, seed = 587)\n",
    "louv_clustering = sorted(louv_clustering, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clust in enumerate(louv_clustering):\n",
    "    print(f\"Cluster {i+1}: {sorted(clust)} \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components = nx.weakly_connected_components(G)\n",
    "for i, component in enumerate(connected_components):\n",
    "    print(f\"Component {i+1}: {component} \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_categ = pd.read_csv('wikispeedia_paths-and-graph/categories.tsv', delimiter='\\t', names=['articles','categories'], header=None, comment=\"#\")\n",
    "article_categ['categories'] = article_categ['categories'].str.split('.').str.get(1)  #to get only the first category after \"subject\"\n",
    "categories_set = set(article_categ['categories'])\n",
    "categories_set.add(\"NOTAG\")\n",
    "print(categories_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_categ.to_csv('couple_art_cat.tsv', sep='\\t', index=False, header=True)\n",
    "no_tag_articles = articles[~articles['article'].isin(article_categ['articles'])]['article'].tolist() #to get the articles that have no category\n",
    "print(no_tag_articles)\n",
    "\n",
    "for art in no_tag_articles:                                                                         #adding them in the article_categ dataframe\n",
    "    new_row = pd.DataFrame({'articles': art, 'categories': [\"NOTAG\"]})\n",
    "    article_categ = pd.concat([article_categ, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_cat_sets = article_categ.groupby('articles', as_index=False)['categories'].apply(lambda x: set(x)) #to remove duplicates\n",
    "art_cat_sets.to_csv('Sets_Art_Cat.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles with multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cardinality = art_cat_sets['categories'].apply(len).max()\n",
    "\n",
    "for i in range(1,max_cardinality+1):\n",
    "    articles_with_i_cat = art_cat_sets[art_cat_sets['categories'].apply(len) == i]\n",
    "\n",
    "    print(\"There are \"+str(len(articles_with_i_cat))+\" articles with cardinality \"+str(i) + \" :\")\n",
    "    if(i==max_cardinality):\n",
    "        print(articles_with_i_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_categories_list(G, clustering, art_cat_sets, toPrint = False):\n",
    "\n",
    "    dict_cat_list = {i: {key: [] for key in categories_set } for i in range(len(clustering))}\n",
    "    for i, clust in enumerate(clustering):\n",
    "        for article in list(G.nodes):\n",
    "            if article in clust:\n",
    "                if article in art_cat_sets['articles'].values:\n",
    "                    for categ in art_cat_sets.loc[art_cat_sets['articles'] == article, 'categories'].values[0]:\n",
    "                        dict_cat_list[i][categ].append(article)\n",
    "    if toPrint:\n",
    "        for clust in dict_cat_list:\n",
    "            print(\"Cluster \" +str(clust+1)+\":\")\n",
    "            for key in dict_cat_list[clust]:\n",
    "                if dict_cat_list[clust][key]:\n",
    "                    print(str(key)+\"\\t\" +str(dict_cat_list[clust][key]))\n",
    "                    print(\"\\n\")\n",
    "            print(\"\\n ################################################################################################################################################################################################## \\n\")\n",
    "    \n",
    "    return dict_cat_list\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_dict = dict_categories_list(G, greedy_clustering, art_cat_sets, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louv_dict = dict_categories_list(G, louv_clustering, art_cat_sets, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_categ_cardinality_per_cluster(alg_dict, toPrint = False):\n",
    "    perc_dict = {clust: {categ: len(alg_dict[clust][categ]) for categ in alg_dict[0] } for clust in range(len(alg_dict))}\n",
    "    \n",
    "    for clust in alg_dict:\n",
    "        print(\"Clustering :\" + str(clust+1))\n",
    "        for categ in alg_dict[clust]:\n",
    "            if toPrint:\n",
    "                print(str(str(categ)+\":\\t\\t\\t\"+\"{:d}\".format(perc_dict[clust][categ])).rjust(40))\n",
    "    return perc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_card_dict = dict_categ_cardinality_per_cluster(greedy_dict,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louv_card_dict = dict_categ_cardinality_per_cluster(louv_dict,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_partial_plot_clustering(card_dict,plot_title,start_pos,end_pos,fig_len, big_val):\n",
    "\n",
    "    data_dicts =  list(card_dict.values())[start_pos:end_pos + 1]\n",
    "    data_dicts\n",
    "    # Number of dictionaries\n",
    "    k = len(data_dicts)\n",
    "    categories = list(data_dicts[0].keys())\n",
    "    x_positions = np.arange(k)  # Positions for bar groups\n",
    "        \n",
    "    num_categories = len(categories)\n",
    "    #cmap = cm.get_cmap('tab20c_r',num_categories)  # You can choose other colormaps like 'viridis', 'plasma', etc.\n",
    "    #colors = [cmap(i / num_categories) for i in range(num_categories)]\n",
    "    # Initialize figure\n",
    "    fig, ax = plt.subplots(figsize=(fig_len,5))\n",
    "    colors = plt.cm.tab20(np.linspace(0,1,num_categories))\n",
    "\n",
    "    # Initialize the bottom tracker for each bar group\n",
    "    bottom_values = [0] * k  \n",
    "\n",
    "    # Plot stacked bars for each category\n",
    "    \n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        heights = [data[category] for data in data_dicts]\n",
    "        ax.bar(x_positions, heights, bottom=bottom_values, label=category, color = colors[i])\n",
    "        # Update the bottom tracker\n",
    "        bottom_values = [bottom + height for bottom, height in zip(bottom_values, heights)]\n",
    "\n",
    "    # Customization\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels([f\"{i+start_pos+1}\" for i in range(k)])\n",
    "    if(big_val):\n",
    "        ax.set_ylim(0, 2000)\n",
    "    else:\n",
    "        ax.set_ylim(0, 60)\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(\"Clusters\")\n",
    "    ax.set_ylabel(\"Cumulative categories cardinality\")\n",
    "    ax.legend(title=\"Categories\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('images/clustering/'+plot_title+'.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_partial_plot_clustering(greedy_card_dict,\"Greedy Clusters pt1\",0,2,6,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_partial_plot_clustering(louv_card_dict,\"Louvian Clusters pt1\",0,7,6,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_partial_plot_clustering(greedy_card_dict,\"Greedy Clusters pt2\",3,len(greedy_card_dict),12,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_partial_plot_clustering(louv_card_dict,\"Louvian Clusters pt2\",8,len(louv_card_dict),12,False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
